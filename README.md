# Category-Theory-Functional-Programming
In this repository, I collect my code and notes that accrue as I read "Category Theory for Programmers" by Bartosz Milewski. The most fun for me are the Challenge sections, which you will find answered in Markdown files as well as in comments in the Haskell and C++ files.


1. **[Identity_Composition](./Identity_Composition/)**
    * The very first chapter, we explore the two key features in a category: **Identity** and **Compostion**.
    * My favorite work in this folder is the general composition functions we defined in [Haskell](./Identity_Composition/Identity_Composition.hs) and [C++](./Identity_Composition/Identity_Composition.cpp). These take in two functions of type **A -> B** and **B -> C** and return one of type **A -> C**. The beauty here (and in most of this whole repo) is that what we really care about is *compilation*, not runtime. We are working with these typed languages to get our hands on the Set of Types as a category to explore.

2. **[Categories](./Categories/)**
    * Here we are introduced to **Monoids** and their associativity, **Hom-sets**, **Ordering** of sets, and other great ideas. 
    * Here I began to see the beauty and power of the exercise of writing things *both* in C++ and in Haskell. It sharpens my programming thinking for sure, but looking at Category-Theoretical structures through the two often very different lenses of **declarative** and **imperative** programming allows the common core beneath to become very firm. 

3. **[Types_Functions](./Types_Functions/)**
    * In this section, we explore **Types as Sets**, among other things. 
    * The best exercise was creating a general **[memoize function in C++](./Types_Functions/Types_Functions.cpp)** that takes in a pure function, *f*, and returns a new function that does exactly what *f* does, but keeps an internal memo of the outputs for past seen values. While the syntax is far more cumbersome than in other languages, I find **C++ Lambdas** beautiful. C++ is emphatically an object-oriented, imperative language, but its strong type system lets you really explore the declarative side of things. In this folder and in those that follow, I do. 

4. **[Kleisli_Embellishments](./Kleisli_Embellishments/)**
    * This was an extremely powerful chapter and problem set. First off, we did everything in both **[Haskell](./Kleisli_Embellishments/Kleisli_Embellishments.hs)** and **[C++](./Kleisli_Embellishments/Kleisli_Embellishments.cpp)**. For this set of functions in particular, this was harder but the payoff was steep!
    * Kleisli Categories, I am to understand, are ones whose morphisms do something embellished or special. I know they are under the umbrella of **Monads**, but Bartosz has warned that we're not quite yet ready for the full thrust of such a definition. What they are to me are wrapper types that allow you to define a family of functions that work on these types as if they are what live inside the wrapper, but also do something special. In other words, they make a special category with special objects and special morphisms. 
    * Our prime example was the **Writer Monad**. This is a pure functional solution to the side-effect-laden pattern of **logging**. When you have a complex program, it becomes helpful for each function to just stream a snippet of text or some other bit of information to a central log. But with functional programming we do not want side effects; we do not want anything to be changed that is not obvious in the inputs and outputs. Really we don't want any *thing* to *be* and then to *change*. We want things to become other things. 
    * We implement **Writer** and **Partial** monads in both languages. It is a real joy. 
    * **C++** templates are just tremendous. As are Lambdas. 
        * *compilation as proof!*

5. **[Products_Coproducts](./Products_Coproducts/)**
    * "We're in the great game now."
    * If category theory before this chapter was a cool way to think about programs, maybe to help you write better ones (like the huge generalization and improvement I made to my **C++** code running the game **Portal** after reading the **[Kleisli_Embellishments](./Kleisli_Embellishments/)** chapter), then after this it became a way of thinking, a powerful obsession, and a propellor of intellectual excitment. 
    * The idea of a **Product** made sense. That of a **Coproduct** made less so, but still compiled. When we got to the notion of *ranking* products in different categories, proving certain ones are better or worse, it got hairy. Likewise, the **Initial Object** and **Terminal Object** made sense on the surface, but Bartosz began discussing the beauty, power, and merits behind these things and I was lost...The challenge where I was specifically struggling was the **[proofs that certain C++ coproducts were better or worse than others](./Products_Coproducts/Products_Coproducts.cpp)**. 
    * I was getting so confused, so tangled in the weeds of the specific problem. I had to step away. Then I got into conversation with a friend, explaining to him a bit of the taste of category theory I had tried. And in discussing its motivations---really at this point just pretrained on Bartosz's words, not *feeling* them myself---it all aligned powerfully. Universal constructions are the boats we build on the Categorical bay so that they fit in so many foreign ports! The boats then must be vague, soft, and adaptable. But they are defined by a wrought-iron skeleton. If our boat fits, we have found the construction. But not all dockings are equal. We can fit into a dock that is too big. Sure, the constraints are met, but have we really found a dock that embodies our boat? No. We want only to dock where there is *fit*. 
    * We define constructions and we define the way to rank them. Then we can use them !
    * On the **C++** problem, again, I was too focused on the details of **C++**, but when I stepped back and just thought minimally about the definitions I needed to satisfy, it all clicked. 
    * I implore the curious visitor to check out this folder. 

6. **[Algebraic_Types](./Algebraic_Types/)**
    * Following directly from the previous chapter, we generalize the alegbraic types and begin to have fun with the direct correspondence with algebra itself. **Product Types** obey the same associativity as the **Product Monoid** and its *unit* is **()**, the singleton set, **1**! **Sum Types** obey the same associativity as the **Sum Monoid** and its *unit* is **Void**, the empty set, **0**!
    * Two concepts became extremely firm for me in this chapter: **isomorphism** and **the ubiquity of Sum-of-Product types**
        * Isomorphism is a notion I have felt intuitively since reading *GÃ¶del, Escher, Bach*, but in there it was admittedly a casual employment of the idea. With **Haskell** and category theory, we can be extremely precise. **Isomorphism means invertible morphism**. If you can write two inverse morphisms between two ***anythings*** then they are isomorphic. A one-to-one, full-fidelity mapping. We do not need ***absolute equality*** but just ***effective equality***. I don't need two types to **be** the same, but if I can go from one to the other without ever losing information or doing extra work, then they're effectively the same thing. 
            * The best exercises of isomorphism in this chapter were the proofs that familiar algebraic relationships such as: **A x (B + C) = A x B + A x C** are perfectly preserved in **Product of Sum** types and **Sum of Product** types. I implore your **[exploration](/Algebraic_Types/Algebraic_Types.hs)**. 
        * The next appreciation cuts far deeper. I have been interested in **Disjunctive Normal Form** and **Sigma Pi Units** in neural networks. These both refer to the notion of a **sum of products** or a **disjunction of conjunctions**, or **"(this AND this) OR (that AND that) OR (this AND that)"**. Whenever you are saying a statement like this, you are defining a concept that is satisfied by one of a set of possible conditions being met in full. In a **SigmaPi neuron**, you are scanning for the presence of some concept. Even when you take the dot product, you're taking a special case of a **SigmaPi** calculation: checking for a concept. This was relatively abstract in my head. Then, since working through some incredible examples of the correspondence between **[Sum Product Types in Haskell](./Algebraic_Types/Algebraic_Types.hs)** and **[Class Hierarchies in C++](./Algebraic_Types/Algebraic_Types.cpp)**, the clouds parted.
            * For example, a **Shape** is either a **Circle(with a radius)**, a **Rectangle(with a width and height)**, or (and there are obviously more) a **Square(with a scale or side-length)**. To *be* a **Shape**, you must fulfill one of these options (defined by what data you have). However no matter what, *once you are a **Shape***, you get an **Area() -> Float** function. 
            * Sums of products (or products of sums, because they are the same thing!) can so well define what we know abstractly as concepts. 
            * Please check it out. 

    * Finally, something beautiful happened when I did the challenge of showing that **a + a = 2 x a holds for types**. You can find this moment of discovery **[here](/Algebraic_Types/Algebraic_Types.cpp)**. 

7. **[Functors](./Functors/)**
    * This is a key chapter in the book; functors are the first meta step up from morphisms between objects to morphisms between categories! This is hard to even phrase! As always, the programming makes all of this vert concrete. The fact that we can explore category theoretical ideas with the category the set of types by writing compilable programs is tremendous. 
    * Functors, like categories, have strict rules. Again, a functor maps a category to a category. This means:
        * Objects mapped to Objects
        * Morphisms mapped to Morphisms
    * Such that the two key features of the two categories are preserved: **identity** and **composition**. Functors can also map objects and morphisms within a category; these are called **endofunctors**. In the category of types and functions, we can, for example, map the set of types to that of parameterized types. Thus **collections such as List or wrappers such as Maybe** can be seen as endofunctors. A list maps type a to a list of type a. To prove it is a functor, we have to prove that we can define a **fmap :: (a -> b) -> List a -> List b** function that can be interpreted in two ways:
        * mapping a function from **a -> b** to a function that maps a **List a -> List b**
        * mapping a function from **a -> b** *and* a **List a** to a **List b**
    * It is clear that these alternatives are just a question of how curried you see this function. In Haskell and composable category theory, they are the same !
    * This chapter also introduces **Equational Reasoning** to make proofs. In **[Haskell](./Functors/Functors.hs)**, I use it to prove several functors. One of which is a cool inductive proof for **Lists**. All that **Equational Reasoning** means is working through substitutions of two patterns on either side of Haskell's assignment operator (=). Check it out. 
        * We also see **typeclasses** in **[Haskell](./Functors/Functors.hs)**: a beautiful way to define Java-like interfaces *and* assign types to these interface trees whenever you want. For the purposes of Functors, we define the Functor type class. The function that needs to be implemented is **fmap**!
    * The coolest code that we wrote in this chapter is at the end of the **[C++](./Functors/Functors.cpp)** file where we define the **Reader Functor** that essentially just uses composition. It maps a type to a function that returns that type. We prove that Reader is a functor in **[Haskell](./Functors/Functors.hs)** equational reasoning proofs. Because of the limits of C++ template templates (you cannot partially specialize templates), we use the uncurried version of **fmap** to map a function from **A -> B** and a function **R -> A** to produce a function **R -> B**. All that **fmap** is is just composition: **R -> A -> B = R -> B**!

8. **[Functoriality](./Functoriality/)**
    * This chapter was very very difficult, despite the unassuming transition that Bartosz made in the beginning: *just a generalization of functors*. Without the **[Haskell](./Functors/Functoriality.hs)** exercises and the need for type-matching, this stuff would not have congealed at all. Bifunctors in English make sense; the general Bifunctor implementation in Haskell makes no sense; a concrete Bifunctor implementation in **[Haskell](./Functors/Functoriality.hs)** or **[C++](./Functors/Functoriality.cpp)** makes a lot of sense. Simply put: if you have mappings from **a -> c** and **b -> d**, then you can map the product category **C = a x b** to then you can use those mappings, **a -> c** and **b -> d**, to get to the category **D = c x d**. It is really just applying to functors in parallel.
    * We also learned about **Covariance** and **Contravariance**. **Contravariance** refers to when a functor maps objects to objects, but flips all arrows in its mapping. Instead of **C -> FC**, it is **C -> C_op -> FC_op**. Further, **Profunctors** are **Bifunctors** who are **Contravariant** in their first argument (mapping from **a -> c**) and **Covariant** in their second. 

* **[DP Graph Shortest Path](/DP_Graph/)**
    * This was an extremely special exercise. Egged on by the professor who pushed me to read **Category Theory for Programmers** further challenged me, now that I am getting used to Haskell, to try to compute the DP solution to the shortest path in a graph. 
    * As always (and especially for a problem with lots of state), I started in **[C++](./DP_Graph/DP_Graph.cpp)**. I tried to keep in mind that I wanted to then go and make this functional, but I started just thinking imperatively. I first implemented a recursive solution with no DP. Then I remembered the **[memoize() function from Types_Functions.cpp](./Types_Functions/Types_Functions.cpp)** and realized that if I memoized my recursive solution, that it would become much more efficient. If you run the **[C++](./DP_Graph/DP_Graph.cpp)** file, you'll see the first run (which uses the memo) and then the non-memo version. In the first, we only call **minDist** on each node once! In the latter, we call it many many times. 
    * Then I moved to **[Haskell](./DP_Graph/DP_Graph.hs)**...I first just wanted to get myself familiar with building, changing, and accessing recursively defined data structures. I knew that all of the persistent state in my **[C++](./DP_Graph/DP_Graph.cpp)** code would need to be replaced by pure functions who pass things around. However, even the simple luxury of a for loop in **[C++](./DP_Graph/DP_Graph.cpp)**, I realized, had to be accounted for with more functions! 
    * I thought I would use a kind of monad to keep track of the path as I went through, but decided first to just get the distance, not worrying about recovering the path. I realized that I could just keep track of the *length* of the shortest path and a memo map as a tuple. This was a big conceptual jump: I can actually structure this somewhat similarly to **[C++](./DP_Graph/DP_Graph.cpp)**, but instead of the internal stateful memo in the function, we just pass the map around. I wrote the main orchestrating function to chew through the graph until we reach the end (until from == to). The base case is nice and easy, then in the recursive case we need to call **minDist** on all children then take their min. So we need a **minInt** function and a function which will loop through children and call **minDist** on them! It is this co-recursion that I find so beautiful. A hierarchy of co-dependent recursive calls. Tremendous. 
    * It turns out that even in that function to call **minDist** on each child, we would not yet be calling **minDist**. We need to use our memo, so we write a **getOrCompute** function. This is where we get our DP and this is where **minDist** is actually called! We get the current edge's full distance, add it to a list-distances-of-the-original-caller's-children, which is then reduced by **minInt**. 
    * I will not spell the whole thing out; look at **[the code](./DP_Graph/DP_Graph.hs)**! But the final beautiful piece was when I realized that it would only be satisfying to get the path itself, not just the distance. I was annoyed because I thought I'd have to rework the whole algorithm, but no! The **MemoMap** we create contains all of the information for each node! So we can now traverse greedily forward through the ***MemoMap***! Just fantastic stuff. 